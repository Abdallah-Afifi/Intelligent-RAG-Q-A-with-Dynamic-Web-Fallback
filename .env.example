# LLM Configuration
# Choose one of the following options:

# Option 1: Groq (Free Tier - Recommended for cloud)
# Get your free API key from: https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here
LLM_PROVIDER=groq
LLM_MODEL=llama-3.1-8b-instant

# Option 2: Ollama (Local - Recommended for privacy)
# Install Ollama from: https://ollama.ai
# LLM_PROVIDER=ollama
# LLM_MODEL=llama3.2
# OLLAMA_BASE_URL=http://localhost:11434

# Option 3: HuggingFace (Free Inference API)
# HUGGINGFACE_API_KEY=your_hf_api_key_here
# LLM_PROVIDER=huggingface
# LLM_MODEL=mistralai/Mistral-7B-Instruct-v0.2

# Embeddings Configuration
EMBEDDING_MODEL=all-MiniLM-L6-v2
EMBEDDING_DEVICE=cpu

# Vector Store Configuration
VECTOR_STORE_PATH=./chroma_db
COLLECTION_NAME=knowledge_base

# RAG Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RETRIEVAL=5
RELEVANCE_THRESHOLD=0.6

# Web Search Configuration
MAX_SEARCH_RESULTS=5
WEB_SEARCH_TIMEOUT=10

# System Configuration
LOG_LEVEL=INFO
CACHE_ENABLED=true
